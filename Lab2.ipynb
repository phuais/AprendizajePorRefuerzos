{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "## Actividades\n",
    "\n",
    "1. Crear tu propio entorno y entrenar agentes RL en el mismo. Analizar la convergencia con distintos algoritmos* (ej: PPO, DQN), resultados con distintas funciones de recompensa e híper-parámetros. \n",
    "\n",
    "    Algunas ideas:\n",
    "\n",
    "    * Transformar GoLeftEnv en una grilla 2D, añadir paredes / trampas / agua.\n",
    "    * Crear un entorno que juegue a algún juego como el ta-te-ti.\n",
    "    * Crea un entorno totalmente nuevo que sea de tu interés!\n",
    "\n",
    "2. Entrena agentes en entornos más complejos con stable-baselines/rl-baselines-zoo. Tener en cuenta:\n",
    "\n",
    "    * Google Colab tiene una limitante en cuanto a cantidad de recursos de CPU/GPU (incluido un \"rendimiento decreciente silencioso\"), lo cuál reduce la capacidad de entrenar distintos entornos.\n",
    "    * Si el entorno no está implementado en stable-baselines, debe hacerse un wrapper a mano, lo que puede ser sencillo o puede llevar algo más de trabajo, teniendo que tocar código subyacente de la librería. \n",
    "\n",
    "\\* pueden ser usando stable-baselines/rl-baselines-zoo o bien utilizando algún otro algoritmo (incluso tabular)\n",
    "\n",
    "## Ejercicio 1: entorno personalizado - Tiro parabólico\n",
    "\n",
    "A continuación se presenta un entorno en donde el agente debe aprender cuál es el ángulo óptimo para tener un alcance igual al de un objetivo predefinido. La fórmula física para el tiro parabólico es la siguiente:\n",
    "\n",
    "$X=\\frac{v_{i}^{2}sen(2\\alpha)}{g}$\n",
    "\n",
    "En donde $v_{i}^{2}$ es la velocidad inicial, $\\alpha$ el ángulo de tiro y $g$ es la gravedad.\n",
    "\n",
    "El agente inicia con un ángulo aleatorio (entre varios múltiplos de pi/180, para mayor simplicidad) y puede ejecutar 4 acciones: dos aumentan y dos disminuyen el ángulo de tiro, de manera brusca o fina. El agente percibe recompensa al tener el alcance igual al objetivo, y tiene recompensa negativa por cada paso que no llega a dicho ángulo.\n",
    "\n",
    "Aplicamos PPO y A2C y observamos el desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "#from gym.envs.registration import register\n",
    "\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiroParabolico(gym.Env):\n",
    "  \"\"\"\n",
    "  Ambiente personalizado que sigue la interfaz de gym.\n",
    "  Es un entorno en el cual el agente debe aprender el a obtener el \n",
    "  ángulo adecuado para llegar a un objetivo mediante un tiro parabólico,\n",
    "  a una velocidad constante de 60 m/s.\n",
    "  \"\"\"\n",
    "  # Dado que estamos en colab, no podemos implementar la salida por interfaz \n",
    "  # gráfica ('human' render mode) \n",
    "  metadata = {'render.modes': ['console']}\n",
    "\n",
    "  def __init__(self):\n",
    "    super(TiroParabolico, self).__init__()\n",
    "    \n",
    "    # Velocidad inicial\n",
    "    self.init_speed = 60\n",
    "    # Ángulos posibles para el cual se calcula la posición del objetivo\n",
    "    self.possible_angles = np.arange(np.pi/180, np.pi/2, np.pi/45)\n",
    "    # Posiciones del objetivo posibles, de acuerdo a los ángulos posibles\n",
    "    self.possible_x = ((self.init_speed ** 2) * np.sin(2*self.possible_angles)) / 9.8\n",
    "    \n",
    "    # 4 acciones: aumentar o disminuir el ángulo, de manera \n",
    "    # brusca o fina\n",
    "    n_actions = 4\n",
    "    self.action_space = spaces.Discrete(n_actions)\n",
    "    \n",
    "    # Espacio de observacion continuo: entre 0 y pi/2 (90 grados sexagesimales)\n",
    "    self.observation_space = spaces.Box(0, np.pi/2, shape=(1,), dtype=np.float32)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Importante: la observación devuelta debe ser un array de numpy\n",
    "    :return: (np.array) \n",
    "    \"\"\"\n",
    "    self.angle = random.choice(self.possible_angles)\n",
    "    self.target_x = random.choice(self.possible_x)\n",
    "    \n",
    "    return np.array(self.angle).astype(np.float32)\n",
    "\n",
    "  def step(self, action):\n",
    "    \n",
    "    # Si el ángulo se va de rango (mayor a pi/2 o menor a 0),\n",
    "    # que defina un nuevo ángulo aleatoriamente\n",
    "    # Sino, que aumente o disminuya el ángulo\n",
    "    # de manera brusca (pi/9) o fina (pi/180)\n",
    "    if (self.angle >= np.pi/2) | (self.angle <= 0):\n",
    "        #done = True\n",
    "        self.angle = random.choice(self.possible_angles)\n",
    "    else:\n",
    "        if action == 0:\n",
    "          self.angle -= np.pi/9\n",
    "        elif action == 1:\n",
    "          self.angle -= np.pi/180\n",
    "        elif action == 2:\n",
    "          self.angle += np.pi/9\n",
    "        elif action == 3:\n",
    "          self.angle += np.pi/180\n",
    "    \n",
    "    # Alcance obtenido con el ángulo elegido\n",
    "    self.alcance = ((self.init_speed ** 2) * np.sin(2*self.angle)) / 9.8\n",
    "    \n",
    "    # Recompensa positiva al llegar al objetivo, o negativa\n",
    "    # en cada accion si no llega. Penaliza que recorra\n",
    "    # muchos ángulos antes de obtener el óptimo\n",
    "    if self.alcance == self.target_x:\n",
    "        reward = 100\n",
    "        done = True\n",
    "    else:\n",
    "        reward = -1\n",
    "        done = False\n",
    "        \n",
    "    info = {}\n",
    "\n",
    "    return np.array([self.angle]).astype(np.float32), reward, done, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # en nuestra interfaz de consola, presentamos el alcance obtenido\n",
    "    print(\"Posicion objetivo =\", self.target_x, \"Alcance aprendido =\", self.alcance)\n",
    "\n",
    "  def close(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizaje con PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/miniconda3/envs/diplodatos/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# PPO\n",
    "\n",
    "env = TiroParabolico()\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=0, tensorboard_log='tensorboard_c/', seed = 42).learn(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C\n",
    "\n",
    "env = TiroParabolico()\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=0, tensorboard_log='tensorboard_c/', seed = 42).learn(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 245.8030798869274\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 12.820223278469644\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = -226.16135828289495\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 353.11654136509674\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 335.5881272972819\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 161.03429882047737\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = -88.86926777130658\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 12.820223278469744\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 245.8030798869275\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 363.7719436193523\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 311.5278720574626\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 113.51644691324601\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = -137.61058533645746\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 12.820223278469744\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 245.8030798869275\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 363.7719436193523\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 311.5278720574626\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 113.51644691324601\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = -137.61058533645746\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 12.820223278469744\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 245.8030798869275\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 363.7719436193523\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 311.5278720574626\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = 113.51644691324601\n",
      "Posicion objetivo = 281.40408114574706 Alcance aprendido = -137.61058533645746\n",
      "Posicion objetivo = 12.820223278469744 Alcance aprendido = 281.40408114574706\n",
      "Posicion objetivo = 335.5881272972819 Alcance aprendido = -88.86926777130658\n",
      "Posicion objetivo = 335.5881272972819 Alcance aprendido = 281.404081145747\n",
      "Posicion objetivo = 335.5881272972819 Alcance aprendido = 63.78912648989275\n",
      "Posicion objetivo = 335.5881272972819 Alcance aprendido = -183.67346938775515\n",
      "Posicion objetivo = 335.5881272972819 Alcance aprendido = 281.404081145747\n",
      "Posicion objetivo = 335.5881272972819 Alcance aprendido = 63.78912648989275\n",
      "Posicion objetivo = 335.5881272972819 Alcance aprendido = -183.67346938775515\n",
      "Posicion objetivo = 335.5881272972819 Alcance aprendido = 353.11654136509674\n",
      "Posicion objetivo = 245.8030798869274 Alcance aprendido = 335.5881272972819\n",
      "Posicion objetivo = 113.51644691324601 Alcance aprendido = -38.39821099628087\n",
      "Posicion objetivo = 113.51644691324601 Alcance aprendido = 335.5881272972819\n",
      "Posicion objetivo = 113.51644691324601 Alcance aprendido = 353.11654136509674\n",
      "Posicion objetivo = 113.51644691324601 Alcance aprendido = 205.4178012749682\n",
      "Posicion objetivo = 113.51644691324601 Alcance aprendido = -38.39821099628087\n",
      "Posicion objetivo = 311.5278720574626 Alcance aprendido = 113.51644691324601\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 311.5278720574626\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 363.77194361935227\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 245.8030798869274\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 12.820223278469644\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = -226.16135828289495\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 311.5278720574626\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 113.51644691324601\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = -137.61058533645746\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 281.404081145747\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 63.78912648989275\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = -183.67346938775515\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 281.404081145747\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 63.78912648989275\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = -183.67346938775515\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 335.5881272972819\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 161.03429882047737\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = -88.86926777130658\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 245.8030798869274\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 12.820223278469644\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = -226.16135828289495\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 363.7719436193523\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 311.5278720574626\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 113.51644691324601\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = -137.61058533645746\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 281.40408114574706\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 367.3469387755102\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 281.404081145747\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 63.78912648989275\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = -183.67346938775515\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 281.40408114574706\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 367.3469387755102\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 281.404081145747\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = 63.78912648989275\n",
      "Posicion objetivo = 205.4178012749682 Alcance aprendido = -183.67346938775515\n",
      "Posicion objetivo = 353.1165413650967 Alcance aprendido = 205.4178012749682\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 63.78912648989275\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = -183.67346938775515\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 311.5278720574626\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 363.77194361935227\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 245.8030798869274\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 12.820223278469644\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = -226.16135828289495\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 335.5881272972819\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 161.03429882047737\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = -88.86926777130658\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 113.51644691324596\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 311.5278720574626\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 363.77194361935227\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 245.8030798869274\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 12.820223278469644\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = -226.16135828289495\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 113.51644691324596\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 311.5278720574626\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 363.77194361935227\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 245.8030798869274\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 12.820223278469644\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = -226.16135828289495\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = 63.78912648989275\n",
      "Posicion objetivo = 245.8030798869275 Alcance aprendido = -183.67346938775515\n"
     ]
    }
   ],
   "source": [
    "if not IN_COLAB:\n",
    "    obs = env.reset()\n",
    "    for i in range(100):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        env.render(mode='console')\n",
    "        if dones[0]:\n",
    "            obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra un ejemplo de cómo el agente llega a la posición objetivo (PPO), ajustando el ángulo:\n",
    "\n",
    "Posicion objetivo = 311.5278720574626 Alcance aprendido = 63.78912648989275\n",
    "\n",
    "Posicion objetivo = 311.5278720574626 Alcance aprendido = -183.67346938775515\n",
    "\n",
    "Posicion objetivo = 311.5278720574626 Alcance aprendido = 245.8030798869274\n",
    "\n",
    "Posicion objetivo = 311.5278720574626 Alcance aprendido = 12.820223278469644\n",
    "\n",
    "Posicion objetivo = 311.5278720574626 Alcance aprendido = -226.16135828289495\n",
    "\n",
    "Posicion objetivo = 311.5278720574626 Alcance aprendido = 335.5881272972819\n",
    "\n",
    "Posicion objetivo = 311.5278720574626 Alcance aprendido = 161.03429882047737\n",
    "\n",
    "Posicion objetivo = 311.5278720574626 Alcance aprendido = -88.86926777130658\n",
    "\n",
    "Posicion objetivo = 311.5278720574626 Alcance aprendido = 12.820223278469744\n",
    "\n",
    "Posicion objetivo = 311.5278720574626 Alcance aprendido = 245.8030798869275\n",
    "\n",
    "Posicion objetivo = **311.5278720574626** Alcance aprendido = 363.7719436193523\n",
    "\n",
    "Posicion objetivo = 63.789126489892766 Alcance aprendido = **311.5278720574626**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 8131), started 0:00:54 ago. (Use '!kill 8131' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b558e74bca625acd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b558e74bca625acd\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=tensorboard_c/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturas de pantalla de tensorboard\n",
    "\n",
    "Recompensa percibida por episodio\n",
    "\n",
    "PPO\n",
    "\n",
    "![env_reward](env_reward.png)\n",
    "\n",
    "A2C\n",
    "\n",
    "![env_reward](env_reward2.png)\n",
    "\n",
    "Número de pasos por episodio\n",
    "\n",
    "PPO\n",
    "\n",
    "![env_pasos](env_ep_len.png)\n",
    "\n",
    "A2C\n",
    "\n",
    "![env_pasos](env_ep_len2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentarios finales\n",
    "\n",
    "Un análisis preliminar indica que PPO se desempeña mejor que A2C. Sin embargo, se podría profundizar en la búsqueda de un mejor método de recompensa y en un ajuste de hiperparámetros para obtener una curva de recompensa más consistente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: entorno complejo con stable-baselines/rl-baselines-zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ejecutó el ambiente \"Pendulum-v0\" mediante la terminal de Linux (una vez clonado localmente el repositorio stable-baselines/rl-baselines-zoo). En este ambiente, el agente debe aprender a posicionar el péndulo de manera vertical. A continuación, se detallan las ejecuciones en la terminal y se muestran las salidas gráficas obtenidas. \n",
    "\n",
    "### Ejecución del aprendizaje con PPO\n",
    "\n",
    "python train.py --algo ppo --env Pendulum-v0 --tensorboard-log /tmp/stable-baselines/\n",
    "\n",
    "### Salida final del aprendizaje\n",
    "\n",
    "![outputfinal](ultimacorrida.png)\n",
    "\n",
    "### Salida gráfica: recompensa obtenida por episodio\n",
    "\n",
    "python scripts/plot_train.py -a ppo -y reward --env Pendulum-v0_1 -f logs/ -x steps\n",
    "\n",
    "![reward](reward.png)\n",
    "\n",
    "### Captura de pantalla del agente aprendido\n",
    "\n",
    "python enjoy.py --algo ppo --env Pendulum-v0 -f logs/ --exp-id 0\n",
    "\n",
    "A la derecha, se observa que el péndulo ha llegado al objetivo de estar vertical sin caerse.\n",
    "\n",
    "![enjoy](enjoy2.png)\n",
    "![enjoy2](enjoy1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
